{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Embedding?\n",
    "\n",
    "- A *text embedding* is a piece of text converted into a vector.\n",
    "- We can then compare how similar different texts are when they are represented with numbers.\n",
    "\n",
    "### Bag-of-Words\n",
    "\n",
    "- The most simple embedding technique is *bag-of-words*, where we create a text embedding by counting the occurrence of each word and combining them into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vectors: [[1 1 0 0 1 0 1 1 0 1 0 1 0 0]\n",
      " [1 1 1 1 0 1 0 0 2 1 1 0 1 1]]\n",
      "Bag-of-Words: ['at' 'beach' 'but' 'don' 'hang' 'know' 'like' 'out' 'she' 'the' 'think'\n",
      " 'to' 'went' 'where']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "corpus = [\n",
    "    \"I like to hang out at the beach.\",\n",
    "    \"I don't know where she went, but I think she's at the beach.\",\n",
    "]\n",
    "\n",
    "print(f\"Feature Vectors: {bag_of_words.fit_transform(corpus).toarray()}\")\n",
    "print(f\"Bag-of-Words: {bag_of_words.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics\n",
    "\n",
    "- We can use difference metrics to calculate the semantic similarity between different texts after they have been vectorized.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- Unsupervised machine learning technique to identify semantic patterns in a text and extract key topics.\n",
    "- The key idea is that text of a specific topic is more likely to produce certain words more frequently.\n",
    "\n",
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "- LSA is used to find relationships between many documents.\n",
    "- It creates a big matrix where each row represents a unique word, and each column a document.\n",
    "- It then reduces this sparse matrix using *Singular Value Decomposition* while maintaining the relationship between words and documents.\n",
    "- *Cosine similarity* is used to identify the similarity between documents.\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- LDA is a Bayesian network.\n",
    "- Treats each document as a *bag-of-words* and assigns each word to different topics.\n",
    "\n",
    "### LSA vs LDA\n",
    "\n",
    "- LSA identifies relationships between documents while LDA extracts topics from individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modelling with genism (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Term frequency-inverse document frequency (TF-IDF) measures the importance of a word to a specific document.\n",
    "- It is the product of two statistics: *term frequency (TF)* and *inverse document frequency (IDF).\n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "- Term frequency is the relative frequency of a term in a document.\n",
    "- Calculated dividing the number of times the term appears in the document over the total number of terms in the document.\n",
    "  - $t$ - Term\n",
    "  - $d$ - Document\n",
    "  - $f_{t, d}$ - Frequency of term $t$ in document $d$.\n",
    "\n",
    "$$\\text{tf}(t, d) = \\frac{f_{t, d}}{\\sum_{t' \\in d} f_{t', d}}$$\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "- Inverse document frequency measures the amount of information a term provides.\n",
    "- Calculated by dividing the total number of documents by the number of documents that contain the term, and taking the logarithm of the quotient.\n",
    "  - $t$ - Term.\n",
    "  - $d$ - Document.\n",
    "  - $D$ - Set of all documents.\n",
    "  - $N$ - Total number of documents.\n",
    "\n",
    "$$\\text{idf}(t, D) = \\log \\frac{N}{\\{d \\in D : t \\in d\\}}$$\n",
    "\n",
    "### TF-IDF Formula\n",
    "\n",
    "- To calculate TF-IDF, multiply values of TF and IDF.\n",
    "\n",
    "$$\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\cdot \\text{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT example with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-BERT\n",
    "\n",
    "- **Context**: Sometimes we want to encode entire sentences but BERT only creates embeddings for individual words.\n",
    "- Simply averaging the values of the word vectors are ineffective.\n",
    "- SBERT uses a *Siamese network*, meaning each time two sentences are passed independently through the same BERT model.\n",
    "  - So what??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT with sentence_transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic\n",
    "\n",
    "- BERTopic is a [topic modelling](#topic-modelling) algorithm using BERT and consists of five steps listed below:\n",
    "- BERTopic is also modular, meaning that we can swap out different algorithms at different steps, for example we could use Principle Component Analysis (PCA) instead of UMAP for dimensionality reduction.\n",
    "\n",
    "### 1. Embed Documents\n",
    "\n",
    "- Start by converting documents to vectors using Sentence-BERT.\n",
    "\n",
    "### 2. Dimensionality Reduction\n",
    "\n",
    "- The output from Sentence-BERT is a high-dimensionality vector that is difficult to cluster.\n",
    "- Therefore we use the UMAP algorithm to reduce the dimensions of our embeddings while retaining their information.\n",
    "- Dimensionality reduction also helps with visualization, since its impossible to visualize anything more than 3-dimensions.\n",
    "\n",
    "### 3. Cluster Documents\n",
    "\n",
    "- Use the HDBSCAN clustering algorithm to cluster similar documents together.\n",
    "\n",
    "### 4. Bag-of-Words\n",
    "\n",
    "- Because there exist different clustering algorithms that create different types of clusters, we don't want to use the centroid to represent the cluster.\n",
    "- As such, the algorithm compiles all documents in each cluster into a giant document. This giant document now represents the cluster.\n",
    "- The algorithm employs a bag-of-words technique to count all the words in each giant document.\n",
    "- This can be done using Scikit-Learn's `CountVectorizer` for example.\n",
    "\n",
    "### 5. Topic Representation\n",
    "\n",
    "- Finally, we want to assign each cluster a bunch of topics.\n",
    "- We use TF-IDF to find the importance of each word to its respective document and pick the most important words as our topics.\n",
    "- Since we are applying TF-IDF on the cluster itself, the algorithm is called class-based TF-IDF (c-TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract Embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce Dimensionality\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    ")\n",
    "\n",
    "# Step 3 - Cluster Reduced Embeddings\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "# Step 4 - Tokenize Topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create Topic Representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# BERTopic Pipeline\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litreview-BYspwzaP-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
