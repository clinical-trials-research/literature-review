{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "- Unsupervised machine learning technique to identify semantic patterns in a text and extract key topics.\n",
    "- The key idea is that text of a specific topic is more likely to produce certain words more frequently.\n",
    "\n",
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "- LSA is used to find relationships between many documents.\n",
    "- It creates a big matrix where each row represents a unique word, and each column a document.\n",
    "- It then reduces this sparse matrix using *Singular Value Decomposition* while maintaining the relationship between words and documents.\n",
    "- *Cosine similarity* is used to identify the similarity between documents.\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- LDA is a Bayesian network.\n",
    "- Treats each document as a *bag-of-words* and assigns each word to different topics.\n",
    "\n",
    "### LSA vs LDA\n",
    "\n",
    "- LSA identifies relationships between documents while LDA extracts topics from individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modelling with genism (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Term frequency-inverse document frequency (TF-IDF) measures the importance of a word to a specific document.\n",
    "- It is the product of two statistics: *term frequency (TF)* and *inverse document frequency (IDF).\n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "- Term frequency is the relative frequency of a term in a document.\n",
    "- Calculated dividing the number of times the term appears in the document over the total number of terms in the document.\n",
    "  - $t$ - Term\n",
    "  - $d$ - Document\n",
    "  - $f_{t, d}$ - Frequency of term $t$ in document $d$.\n",
    "\n",
    "$$\\text{tf}(t, d) = \\frac{f_{t, d}}{\\sum_{t' \\in d} f_{t', d}}$$\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "- Inverse document frequency measures the amount of information a term provides.\n",
    "- Calculated by dividing the total number of documents by the number of documents that contain the term, and taking the logarithm of the quotient.\n",
    "  - $t$ - Term.\n",
    "  - $d$ - Document.\n",
    "  - $D$ - Set of all documents.\n",
    "  - $N$ - Total number of documents.\n",
    "\n",
    "$$\\text{idf}(t, D) = \\log \\frac{N}{\\{d \\in D : t \\in d\\}}$$\n",
    "\n",
    "### TF-IDF Formula\n",
    "\n",
    "- To calculate TF-IDF, multiply values of TF and IDF.\n",
    "\n",
    "$$\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\cdot \\text{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF with Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT example with transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-BERT\n",
    "\n",
    "- **Context**: Sometimes we want to encode entire sentences but BERT only creates embeddings for individual words.\n",
    "- Simply averaging the values of the word vectors are ineffective.\n",
    "- SBERT uses a *Siamese network*, meaning each time two sentences are passed independently through the same BERT model.\n",
    "  - So what??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT with sentence_transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic\n",
    "\n",
    "- BERTopic is a topic modelling algorithm using BERT.\n",
    "- The algorithm consists of five steps:\n",
    "  - Generate embeddings with Sentence-BERT.\n",
    "  - Dimensionality reduction with UMAP to reduce the dimensions of the output embeddings from Sentence-BERT for easy visualization.\n",
    "  - Clustering with HDBSCAN.\n",
    "  - Tokenizing with CountVectorizer to find the most representative words for each topic.\n",
    "  - Weighting with c-TF-IDF.\n",
    "  - Optional representation tuning.\n",
    "\n",
    "\n",
    "### Embed Documents\n",
    "\n",
    "- Start by converting documents to vectors using Sentence-BERT.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "- The output from Sentence-BERT is a high-dimensionality vector that is difficult to cluster.\n",
    "- Therefore we use the UMAP algorithm to reduce the dimensions of our embeddings while retaining their information.\n",
    "- Dimensionality reduction also helps with visualization, since its impossible to visualize anything more than 3-dimensions.\n",
    "\n",
    "### Cluster Documents\n",
    "\n",
    "- Use the HDBSCAN clustering algorithm to cluster similar documents together.\n",
    "\n",
    "### Bag-of-Words\n",
    "\n",
    "- Because there exist different clustering algorithms that create different types of clusters, we don't want to use the centroid to represent the cluster.\n",
    "- As such, the algorithm compiles all documents in each cluster into a giant document. This giant document now represents the cluster.\n",
    "- The algorithm employs a bag-of-words technique to count all the words in each giant document.\n",
    "- This can be done using Scikit-Learn's `CountVectorizer` for example.\n",
    "\n",
    "### Topic Representation\n",
    "\n",
    "- Finally, we want to assign each cluster a bunch of topics.\n",
    "- We use TF-IDF to find the importance of each word to its respective document and pick the most important words as our topics.\n",
    "- Since we are applying TF-IDF on the cluster itself, the algorithm is called class-based TF-IDF (c-TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple examples using different configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litreview-BYspwzaP-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
